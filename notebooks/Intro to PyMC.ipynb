{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZQIN-jg1BqJ1"
   },
   "source": [
    "# Introduction to PyMC\n",
    "PyMC is an open-source probabilistic programming library in Python, tailored for constructing and estimating Bayesian models. By leveraging PyTensor, PyMC enables efficient computation through automatic differentiation and dynamic compilation to various computational backends, including C, JAX, and Numba, thereby optimizing performance across diverse hardware architectures.\n",
    "\n",
    "The library offers an intuitive and expressive syntax that closely mirrors the natural mathematical notation used in statistical modeling, facilitating the seamless translation of theoretical models into executable code. This design choice enhances readability and maintainability, allowing practitioners to focus on model development without being encumbered by low-level implementation details.\n",
    "\n",
    "PyMC incorporates state-of-the-art inference algorithms, notably the No-U-Turn Sampler (NUTS), an adaptive variant of Hamiltonian Monte Carlo (HMC), which excels in sampling from complex, high-dimensional posterior distributions. Additionally, it supports variational inference methods, providing flexibility in choosing the most appropriate inference technique based on model requirements and computational considerations.\n",
    "\n",
    "Beyond its core modeling and inference capabilities, PyMC integrates seamlessly with ArviZ, a library dedicated to exploratory analysis of Bayesian models. This integration facilitates comprehensive model checking, diagnostics, and visualization, thereby enhancing the robustness and transparency of statistical analyses.\n",
    "\n",
    "The versatility of PyMC extends to a wide array of applications, encompassing generalized hierarchical linear regression, time series analysis, ordinary differential equation modeling, and non-parametric approaches such as Gaussian processes. Its adaptability and comprehensive feature set make it an invaluable tool for addressing complex statistical modeling challenges across various scientific and engineering disciplines.\n",
    "\n",
    "For those interested in exploring PyMC further, the official documentation provides extensive resources, including tutorials and example notebooks, to assist users in effectively applying the library to their specific modeling needs.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q-WS6CWJC-9O"
   },
   "source": [
    "## PyMC Basics: Models and Random Variables\n",
    "In order to get the most out of PyMC, we need to learn about its basic building blocks: models and random variables.\n",
    "\n",
    "The examples below are inspired and adapted (and sometimes just copied) from the excellent [PyMC documentation](https://www.pymc.io/welcome.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FL4qFiQEDkp0"
   },
   "outputs": [],
   "source": [
    "import arviz as az\n",
    "import pandas as pd\n",
    "import pymc as pm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jVfLl5q8Dsv_",
    "outputId": "9310c645-fe79-42e4-ddab-98e92f1bb1b2"
   },
   "outputs": [],
   "source": [
    "print(f\"Using PyMC version: {pm.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9-tqezW-D6dU"
   },
   "source": [
    "The line above should print something like `5.19.1`.\n",
    "\n",
    "Now, let's generate some data. We'll draw 100 observations from a Normal random variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "h4MdP2_-EKGI",
    "outputId": "2b335856-cc73-4a29-ad91-e10bed17dc66"
   },
   "outputs": [],
   "source": [
    "rng = np.random.default_rng(42)\n",
    "data = rng.standard_normal(100)\n",
    "print(f\"Observed data with mean {np.mean(data)} and sd {np.std(data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rul3CyhkEVew"
   },
   "source": [
    "In order to use PyMC, we need to create a probabilistic model and populate it with free random variables and observed variables.\n",
    "\n",
    "We create a model by using the context manager `with`: every random variable defined within the context manager will be associated with the model.\n",
    "\n",
    "Below, we create a free random variable that will be the average of the observed data we have just generated. Then, we'll try and infer the distribution of the average."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QzXENYdh5R_z"
   },
   "outputs": [],
   "source": [
    "with pm.Model() as model:\n",
    "    mu = pm.Normal(\"mu\", mu=0, sigma=1)\n",
    "    obs = pm.Normal(\"obs\", mu=mu, sigma=1, observed=data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n31V-2V1Eikh"
   },
   "source": [
    "Let's check out the random variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a18XlCOMEkgt",
    "outputId": "3effc91e-e443-4019-b4aa-dff2e23f158a"
   },
   "outputs": [],
   "source": [
    "model.basic_RVs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JdgQFsD9E1OY",
    "outputId": "f9cb2d5c-914d-48e8-ef00-081ea9d4388c"
   },
   "outputs": [],
   "source": [
    "model.free_RVs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4ELmJVGPE2y1",
    "outputId": "001e8a55-189f-4f88-c478-57f772200800"
   },
   "outputs": [],
   "source": [
    "model.observed_RVs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "caI_UFA1FA3y"
   },
   "source": [
    "We have:\n",
    "- one free random variable, whose distribution can be estimated by the sampling process\n",
    "- one observed random variable (the data)\n",
    "\n",
    "We will see other ways of introducing data later.\n",
    "\n",
    "Now, let's check out the variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8-ncENmiHQIw"
   },
   "source": [
    "## Variable transformation\n",
    "When defining a model, you can combine several variables and deterministic transformation, using standard python operators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Uta3Vr9iFSkB"
   },
   "outputs": [],
   "source": [
    "with pm.Model() as model:\n",
    "    x = pm.Normal(\"x\", mu=0, sigma=1)\n",
    "    y = pm.Gamma(\"y\", alpha=1, beta=1)\n",
    "    plus_2 = x + 2\n",
    "    summed = x + y\n",
    "    squared = x**2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5eLXORQpHeF2"
   },
   "source": [
    "However, the transformed variables are not saved when inference is run. In order to save them, you need to use a data container, as in the following cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IbIr6d2CHmC1"
   },
   "outputs": [],
   "source": [
    "with pm.Model():\n",
    "    x = pm.Normal(\"x\", mu=0, sigma=1)\n",
    "    plus_2 = pm.Deterministic(\"x plus 2\", x + 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3XxIUob_H5LM"
   },
   "source": [
    "In order to use higher-order variables, you can exploit the `dims` parameter of each random variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MYkjWBJ3IBfo"
   },
   "outputs": [],
   "source": [
    "coords = {\"cities\": [\"Santiago\", \"Mumbai\", \"Tokyo\"]}\n",
    "with pm.Model(coords=coords) as model:\n",
    "    x = pm.Normal(\"x\", mu=0, sigma=1, dims=\"cities\")\n",
    "    y = x[0] * x[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "whon-5BHIftE"
   },
   "source": [
    "## Inference\n",
    "\n",
    "Now that we know how to populate models, let's try and draw some inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "df65d91bb3f54d24bb79f482f3a938e5",
      "a34d5b3bc3fb41439600e00d7d32cfdf",
      "cd87b8db0fca4dffb2c148c18b967d59",
      "fa5679c00a7848859a0f323eeb0e1afd"
     ]
    },
    "id": "B9rfWJOwIhEf",
    "outputId": "4696a075-9464-4bdc-865d-b84ac8f1602e"
   },
   "outputs": [],
   "source": [
    "with pm.Model() as model:\n",
    "    mu = pm.Normal(\"mu\", mu=0, sigma=1)\n",
    "    sd = pm.HalfNormal(\"sd\", sigma=1)\n",
    "    obs = pm.Normal(\"obs\", mu=mu, sigma=sd, observed=data)\n",
    "\n",
    "    posterior_samples = pm.sample(random_seed=rng)\n",
    "    prior_samples = pm.sample_prior_predictive(samples=1000, random_seed=rng)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "68Wk6OqFJOd3"
   },
   "source": [
    "Oversimplifying, the sampling process estimates the distribution of unobserved variables by running multiple Markov chains whose value distributions converge to the posterior distribution of the unobserved variables.\n",
    "\n",
    "The sampling algorithms can be complicated, but PyMC will handle the complexity for you.\n",
    "\n",
    "Then, we can exploit the integration with arviz to visualize the distributions.\n",
    "\n",
    "First, we'll visualize the prior distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "plt.hist(\n",
    "    np.array(prior_samples.prior[\"mu\"][0, :]),\n",
    "    bins=50,\n",
    "    color=\"cornflowerblue\",\n",
    "    alpha=0.7,\n",
    "    edgecolor=\"white\",\n",
    "    density=True,\n",
    ")\n",
    "\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.title(\"Distribution of Prior Samples (μ)\", fontsize=14, pad=15)\n",
    "plt.xlabel(\"μ Values\", fontsize=12)\n",
    "plt.ylabel(\"Density\", fontsize=12)\n",
    "\n",
    "# Add mean line and annotation\n",
    "mean_mu = np.mean(prior_samples.prior[\"mu\"][0, :])\n",
    "plt.axvline(mean_mu, color=\"red\", linestyle=\"--\", alpha=0.5)\n",
    "plt.text(mean_mu * 1.1, plt.gca().get_ylim()[1] * 0.9, f\"Mean: {mean_mu:.2f}\", fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "plt.hist(\n",
    "    np.array(prior_samples.prior[\"sd\"][0, :]),\n",
    "    bins=50,\n",
    "    color=\"#3498db\",\n",
    "    alpha=0.7,\n",
    "    edgecolor=\"white\",\n",
    "    density=True,\n",
    "    label=\"σ\",\n",
    ")\n",
    "\n",
    "mean_sd = np.mean(np.array(prior_samples.prior[\"sd\"][0, :]))\n",
    "plt.axvline(mean_sd, color=\"#e74c3c\", linestyle=\"--\", label=f\"Mean: {mean_sd:.2f}\", alpha=0.8)\n",
    "plt.text(mean_sd * 1.1, mean_sd * 0.9, f\"Mean: {mean_sd:.2f}\", fontsize=10)\n",
    "\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.title(\"Distribution of Prior Samples (σ)\", fontsize=14, pad=15)\n",
    "plt.xlabel(\"σ Values\", fontsize=12)\n",
    "plt.ylabel(\"Density\", fontsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we check out the sampling process and the posterior distributions.\n",
    "\n",
    "There are no spikes or divergences in the chains, so the inference process was smooth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 389
    },
    "id": "FD8JJKvZJpn_",
    "outputId": "94906848-ec87-431d-a8cc-b532145b2946"
   },
   "outputs": [],
   "source": [
    "az.plot_trace(posterior_samples);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we check out a summary and a more detailed plot for the posteriors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 112
    },
    "id": "Jjcm2P1LKBPk",
    "outputId": "fcb0a2b9-2863-4e60-97a1-a1471a2e915d"
   },
   "outputs": [],
   "source": [
    "az.summary(posterior_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 514
    },
    "id": "4B200eDoKKqx",
    "outputId": "f100977a-2ce9-4321-f564-2c80642774d3"
   },
   "outputs": [],
   "source": [
    "az.plot_posterior(posterior_samples);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i1t3aWb3KaZR"
   },
   "source": [
    "## Prediction\n",
    "Once the posterior distribution has been estimated, it can be used to compute predictions on new samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 33,
     "referenced_widgets": [
      "6dead6aba1b7424995fe32fc52cbc936",
      "5a609a91fc0744cda8fee24f2ad0302b"
     ]
    },
    "id": "E_Wgo77_KjTu",
    "outputId": "ef87f08e-ffa8-4174-bcab-8110d0446760"
   },
   "outputs": [],
   "source": [
    "with model:\n",
    "    posterior_samples.extend(pm.sample_posterior_predictive(posterior_samples, random_seed=rng))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 529
    },
    "id": "Ltil-fWgKmPr",
    "outputId": "af8ee09a-61b9-4973-d82b-7f0c84f0cc3b"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "az.plot_ppc(posterior_samples, ax=ax)\n",
    "ax.axvline(data.mean(), ls=\"--\", color=\"r\", label=\"True mean\")\n",
    "ax.legend(fontsize=10);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1sGfRCZrRNDW"
   },
   "source": [
    "## Linear Regression\n",
    "We consider a simple Bayesian linear regression model with normally distributed priors for the parameters. We are interested in predicting outcomes (Y) as normally distributed observations with an expected value ($\\mu$) that is a linear function of two predictor variables,  ($X_1$) and ($X_2$):\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "Y  &\\sim N(\\mu, \\sigma^2) \\\\\n",
    "\\mu &= \\alpha + \\beta_1 X_1 + \\beta_2 X_2\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "where ($\\alpha$) is the intercept, ($\\beta_i$) is the coefficient for covariate ($X_i$), and ($\\sigma$) represents the observation error.\n",
    "\n",
    "Since we are constructing a Bayesian model, we must assign a prior distribution to the unknown variables in the model. We choose zero-mean normal priors with variance of 100 for both regression coefficients, which corresponds to *weak* information regarding the true parameter values. We choose a half-normal distribution (normal distribution bounded at zero) as the prior for ($\\sigma$).\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\alpha &\\sim N(0, 100) \\\\\n",
    "\\beta_i &\\sim N(0, 100) \\\\\n",
    "\\sigma &\\sim \\lvert N(0, 1)\\rvert\n",
    "\\end{aligned}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HKHJ9xqqykBo"
   },
   "source": [
    "To start, we create a simulated dataset compliant with our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CnpTrhiYxi_v"
   },
   "outputs": [],
   "source": [
    "# True parameter values\n",
    "alpha, sigma = 2, 1\n",
    "beta = 3\n",
    "\n",
    "# Size of dataset\n",
    "train_size = 200\n",
    "\n",
    "# Predictor variable\n",
    "x = rng.normal(size=train_size)\n",
    "\n",
    "# Simulate outcome variable\n",
    "y = alpha + beta * x + rng.normal(size=train_size) * sigma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aKzFrm0GzT-H"
   },
   "source": [
    "Then, we visualize the relationship between the variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 432
    },
    "id": "TCXKVoPoygyf",
    "outputId": "4cb47708-4a93-44b5-d3b0-88b7674a9a74"
   },
   "outputs": [],
   "source": [
    "plt.scatter(x, y, alpha=0.6)\n",
    "plt.title(\"Scatter plot of x vs y\", fontsize=14)\n",
    "plt.xlabel(\"x\", fontsize=12)\n",
    "plt.ylabel(\"y\", fontsize=12)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "589pJaQ4zbYQ"
   },
   "source": [
    "Then, we build the model specification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 339
    },
    "id": "tWlAkYCuzia1",
    "outputId": "9376bfbc-7b18-4466-8d4b-265c3e11db89"
   },
   "outputs": [],
   "source": [
    "with pm.Model() as linear_model:\n",
    "    # Priors for unknown model parameters\n",
    "    alpha_rv = pm.Normal(\"alpha\", mu=0, sigma=10)\n",
    "    beta_rv = pm.Normal(\"beta\", mu=0, sigma=10)\n",
    "    sigma_rv = pm.HalfNormal(\"sigma\", sigma=1)\n",
    "\n",
    "    x_data = pm.Data(\"x\", x.tolist())\n",
    "\n",
    "    # Expected value of outcome\n",
    "    mu = alpha_rv + beta_rv * x_data\n",
    "\n",
    "    # Likelihood (sampling distribution) of observations\n",
    "    y_obs = pm.Normal(\"y\", mu=mu, sigma=sigma_rv, observed=y, shape=x_data.shape)\n",
    "\n",
    "pm.model_to_graphviz(linear_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ynonnWvo0Jgz"
   },
   "source": [
    "Now, let's sample the posterior distributions of the parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 193,
     "referenced_widgets": [
      "9c4c71ba2b8d404fa5ef44a760ac74ef",
      "03e92d6defc449dc869849bb6f62bbc6",
      "540adf0d18f6431caea7dae1f06a147b",
      "91303a559111433fa0d0dc2fb9131bd7"
     ]
    },
    "id": "738wNOCH0JEg",
    "outputId": "b81d2168-561e-4b6d-da37-f60b9ee3fa26"
   },
   "outputs": [],
   "source": [
    "with linear_model:\n",
    "    posterior_samples = pm.sample(return_inferecedata=True, random_seed=rng)\n",
    "\n",
    "posterior_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hvtNJojQ0U5O"
   },
   "source": [
    "Feel free to explore the `idata` object, checking out its attributes, or to sample and plot the priors as we did before.\n",
    "\n",
    "Let's visualize the posterior distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 543
    },
    "id": "akjH2HKy0UVb",
    "outputId": "2cd5e513-4bce-47e8-8816-c429c746b684"
   },
   "outputs": [],
   "source": [
    "az.plot_trace(posterior_samples, combined=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 143
    },
    "id": "fVwazuI22Vra",
    "outputId": "608619e4-ece4-4938-b740-9a4d4c1b3336"
   },
   "outputs": [],
   "source": [
    "az.summary(posterior_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m2zEPxar1NBq"
   },
   "source": [
    "Remember: the priors were highly uninformative, so it is not obvious to get posteriors so close to the true values.\n",
    "\n",
    "Let's try and draw some inference from new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 222,
     "referenced_widgets": [
      "2b27229281ec464d95f839b324893919",
      "f00ee115cabe4fd6974a1f3b5e1b9bab"
     ]
    },
    "id": "O7AIgcJh2ch9",
    "outputId": "db490a03-4e15-4f51-e7a1-8dd1abb51697"
   },
   "outputs": [],
   "source": [
    "test_size = 50\n",
    "\n",
    "test_x = rng.normal(size=test_size)\n",
    "\n",
    "with linear_model:\n",
    "    pm.set_data({\"x\": test_x.tolist()})\n",
    "    prediction_trace = pm.sample_posterior_predictive(\n",
    "        posterior_samples,\n",
    "        var_names=[\"y\"],\n",
    "        return_inferencedata=True,\n",
    "        predictions=True,\n",
    "        extend_inferencedata=True,\n",
    "        random_seed=rng,\n",
    "    )\n",
    "    predictions_mean = prediction_trace[\"predictions\"][\"y\"][1].mean(axis=0)\n",
    "    prediction_intervals = az.hdi(ary=prediction_trace[\"predictions\"], hdi_prob=0.95)[\"y\"]\n",
    "    prediction_df = pd.DataFrame(\n",
    "        {\n",
    "            \"x\": test_x,\n",
    "            \"y\": alpha + beta * test_x + rng.normal(size=test_size) * sigma,\n",
    "            \"prediction_mean\": predictions_mean,\n",
    "            \"prediction_hdi_5\": prediction_intervals[:, 0],\n",
    "            \"prediction_hdi_95\": prediction_intervals[:, 1],\n",
    "        }\n",
    "    )\n",
    "\n",
    "prediction_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lMP2oQ7sBMcA"
   },
   "source": [
    "And now let's visualize the predictions, with their confidence interval.\n",
    "\n",
    "As we are sampling the full posterior distribution of the paramters, the predictions are themselves fully probabilistic, meaning we get a probability distribution for each predicted sample.\n",
    "\n",
    "In the chart, we only represent the mean, and the 5th and the 95th percentile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 449
    },
    "id": "WMHiK_84BO4f",
    "outputId": "50dbecdb-e90d-45d6-e72a-0833b2292ca2"
   },
   "outputs": [],
   "source": [
    "prediction_df = prediction_df.sort_values(\"x\")\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(prediction_df[\"x\"], prediction_df[\"y\"], label=\"Observed y\")\n",
    "plt.plot(\n",
    "    prediction_df[\"x\"],\n",
    "    prediction_df[\"prediction_mean\"],\n",
    "    label=\"Prediction mean\",\n",
    "    color=\"blue\",\n",
    ")\n",
    "plt.fill_between(\n",
    "    prediction_df[\"x\"],\n",
    "    prediction_df[\"prediction_hdi_5\"],\n",
    "    prediction_df[\"prediction_hdi_95\"],\n",
    "    color=\"red\",\n",
    "    alpha=0.2,\n",
    "    label=\"95% HDI\",\n",
    ")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
